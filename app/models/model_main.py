# PS C:\Users\007\Documents\TEAM3_GITHUB\AI> venv\Scripts\activate
# (venv) PS C:\Users\007\Documents\TEAM3_GITHUB\AI> python app/models/model_main.py "app/models/one_imageDetection/London_CourtauldGallery_Manet'sABar.jpg"

# íŒ¨í‚¤ì§€ ì„¤ì¹˜ê°€ ì•ˆ ë˜ì–´ìˆë‹¤ë©´, ì•„ë˜ ì½”ë“œ ì¤‘ í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜.
# pip install opencv-python tensorflow numpy matplotlib
# pip install openai
# pip install requests
# pip install python-dotenv
# pip install langchain langchain-openai openai
# pip install python-dotenv groq
# pip install gTTS


# PS C:\Users\007\Documents\TEAM3_GITHUB\AI> venv\Scripts\activate
# (venv) PS C:\Users\007\Documents\TEAM3_GITHUB\AI> python app/models/model_main.py "app/models/one_imageDetection/London_CourtauldGallery_Manet'sABar.jpg"

# python main.py "app/models/London_CourtauldGallery_Manet'sABar.jpg"
# image_path = "app\models\Van Gogh's The Starry Night.png"  # ì‚¬ìš©ìê°€ ì œê³µí•œ ì´ë¯¸ì§€ ê²½ë¡œ
# image_path = "app\models\London_CourtauldGallery_Cezanne's.png"  # ì‚¬ìš©ìê°€ ì œê³µí•œ ì´ë¯¸ì§€ ê²½ë¡œ
# image_path = "app\models\London_CourtauldGallery_Manet'sABar.jpg"

# main.py
# model_main.py
import sys
import os
from one_imageDetection.opencv_utils import load_and_preprocess_image, detect_edges, extract_dominant_colors, display_results
from three_llm.llm import generate_vlm_description_qwen, generate_rich_description, text_to_speech


if __name__ == "__main__":
    # ğŸ”¹ í…ŒìŠ¤íŠ¸í•  ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
    test_images = [
        "app/models/one_imageDetection/London_CourtauldGallery_Cezanne's.png",
        "app/models/one_imageDetection/London_CourtauldGallery_Manet'sABar.jpg",
        "app/models/one_imageDetection/Van Gogh's The Starry Night.png",
    ]

    for image_path in test_images:
        print(f"\nğŸ” í…ŒìŠ¤íŠ¸ ì¤‘: {image_path}")
        display_results(image_path)

        # ğŸ”¹ OpenCV ë¶„ì„ ì‹¤í–‰
        image = load_and_preprocess_image(image_path)
        edges = detect_edges(image)
        dominant_colors = extract_dominant_colors(image)

       # âœ… Qwen2.5-VL ì‹¤í–‰
        print("\nğŸ¨ Qwen2.5-VL ëª¨ë¸ ì‹¤í–‰ ì¤‘...")
        vlm_descriptions = generate_vlm_description_qwen(image_path)

        # âœ… ê²°ê³¼ê°€ ë¬¸ìì—´ì´ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        if isinstance(vlm_descriptions, str):
            vlm_descriptions = [vlm_descriptions]

        # âœ… ê²°ê³¼ê°€ Noneì´ë©´ ê¸°ë³¸ê°’ ì„¤ì •
        if vlm_descriptions is None:
            vlm_descriptions = ["ì„¤ëª…ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]

        # âœ… ë¦¬ìŠ¤íŠ¸ë¥¼ ì¤„ë°”ê¿ˆìœ¼ë¡œ ì—°ê²°í•˜ì—¬ ì¶œë ¥
        print("\n".join(vlm_descriptions))

        # ğŸ”¹ LLMì„ í™œìš©í•œ ì„¤ëª… ìƒì„±
        rich_description = generate_rich_description("í…ŒìŠ¤íŠ¸ ê·¸ë¦¼", vlm_descriptions[0], dominant_colors, edges)
        print("\nğŸ“œ ìƒì„±ëœ ì„¤ëª…:")
        print(rich_description)

        # ğŸ”¹ ìŒì„± ë³€í™˜ ì‹¤í–‰
        text_to_speech(rich_description, output_file=f"output_{os.path.basename(image_path)}.mp3")
